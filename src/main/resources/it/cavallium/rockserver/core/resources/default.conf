database: {
  metrics: {
    database-name: "default"
    influx: {
      enabled: false
      url: ""
      bucket: ""
      user: ""
      token: ""
      org: ""
      allow-insecure-certificates: true
    }
    jmx: {
      enabled: true
    }
  }
  parallelism: {
    # Maximum read tasks in parallel
    read: 30
    # Maximum write tasks in parallel
    write: 10
  }
  global: {
    # Keep true unless you need to debug the get method or leaks
    enable-fast-get: true
    # Keep false unless you have a legacy database
    enable-column-bug: false
    # Enable to adapt the database to spinning disk
    spinning: false
    # Enable to require absolute consistency after a crash. False to use the PointInTime recovery strategy
    absolute-consistency: true
    # Set this option to true during creation of database if you want to be able
    # to ingest behind (call IngestExternalFile() skipping keys that already
    # exist, rather than overwriting matching keys).
    # Setting this option to true will affect 2 things:
    #   1) Disable some internal optimizations around SST file compression
    #   2) Reserve bottom-most level for ingested files only.
    #   3) Note that num_levels should be >= 3 if this option is turned on.
    # DEFAULT: false
    ingest-behind: false
    # ENABLE THIS ONLY WHEN DOING BULK WRITES, THIS IS UNSAFE TO USE IN NORMAL SCENARIOS
    # Setting unorderedWrite() to true trades higher write throughput
    # with relaxing the immutability guarantee of snapshots.
    # This violates the repeatability one expects from ::Get from a snapshot,
    # as well as ::MultiGet and Iterator's consistent-point-in-time view property.
    # If the application cannot tolerate the relaxed guarantees,
    # it can implement its own mechanisms to work around
    # that and yet benefit from the higher throughput.
    # Using TransactionDB with WRITE_PREPARED write policy and twoWriteQueues() true
    # is one way to achieve immutable snapshots despite unordered_write.
    # By default, i. e., when it is false, rocksdb does not advance the sequence
    # number for new snapshots unless all the writes with
    # lower sequence numbers are already finished.
    # This provides the immutability that we except from snapshots.
    # Moreover, since Iterator and MultiGet internally depend on snapshots,
    # the snapshot immutability results into Iterator
    # and MultiGet offering consistent-point-in-time view.
    # If set to true, although Read-Your-Own-Write property is still provided,
    # the snapshot immutability property is relaxed: the writes issued after
    # the snapshot is obtained (with larger sequence numbers) will be still not
    # visible to the reads from that snapshot, however, there still might be pending
    # writes (with lower sequence number) that will change the state visible
    # to the snapshot after they are landed to the memtable.
    # DEFAULT: false
    unordered-write: false
    # Error checking
    checksum: true
    # Use direct I/O in RocksDB databases (Higher I/O read throughput but OS cache is not used, less swapping, less memory pressure)
    use-direct-io: true
    # Allow memory mapped (mmap) RocksDB databases (High OS cache usage if direct I/O is not enabled)
    allow-rocksdb-memory-mapping: false
    # Maximum open files for each RocksDB database instance. -1 is infinite.
    # If the maximum open files count is -1, the initial startup time will be slower.
    # If "cacheIndexAndFilterBlocks" is false, the memory will rise when the number of open files rises.
    maximum-open-files: -1
    # Optimistic transactions
    optimistic: true
    # Database block cache size
    block-cache: 512MiB
    # Database write buffer manager size
    # You should enable this option if you are using direct I/O or spinning disks
    write-buffer-manager: 128MiB
    # Log data path
    log-path: ./logs
    # Write-Ahead-Log data path
    wal-path: ./wal
    # Write-Ahead-Log data path
    temp-sst-path: ./temp_sst
    # If set and greater than zero, the WAL will not be flushed on every write, but every x seconds
    delay-wal-flush-duration: PT5S
    fallback-column-options: {
      # Maximum SST size for last level. Default is 64MiB
      first-level-sst-size: "64MiB"
      # Maximum SST size for last level. Default is 256GiB
      max-last-level-sst-size: "256MiB"
      # RocksDB data volumes.
      volumes: [
        {
          # Path of the volume
          volume-path: "./volume"
          # Maximum size of the volume. This property is ignored on the last volume
          target-size: "10TiB"
        }
      ]
      # RocksDB data levels
      # Available compression types: PLAIN, SNAPPY, LZ4, LZ4_HC, ZSTD, ZLIB, BZLIB2
      levels: [
        {
          compression: LZ4
          max-dict-bytes: 0
        }
        {
          compression: LZ4
          max-dict-bytes: 0
        }
        {
          compression: ZSTD
          max-dict-bytes: 0
        }
        {
          compression: ZSTD
          max-dict-bytes: 0
        }
        {
          compression: ZSTD
          max-dict-bytes: 0
        }
        {
          compression: ZSTD
          max-dict-bytes: 0
        }
        {
          compression: ZSTD
          # Maximum compression dictionary bytes per-sst
          max-dict-bytes: 32KiB
        }
      ]
      # Memtable memory budget for RocksDB
      # Used to optimize compactions and avoid write stalls
      memtable-memory-budget-bytes: 128MiB
      # Disable to reduce IOWAIT and make the read/writes faster
      # Enable to reduce ram usage
      # If maximum-open-files is != -1, this option must be set to true,
      #  otherwise the indexes and filters will be unloaded often
      cache-index-and-filter-blocks: true
      # Disable to reduce IOWAIT and make the read/writes faster
      # Enable to reduce ram usage
      partition-filters: false
      # Bloom filter.
      bloom-filter: {
        # Bits per key. This will determine bloom memory size: bitsPerKey * totalKeys
        bits-per-key: 10
        # Disable bloom for the bottommost level, this reduces the memory size to 1/10
        optimize-for-hits: false
      }
      # Use relatively larger block sizes to reduce index block size.
      # You should use at least 64KB block size.
      # You can consider 256KB or even 512KB.
      # The downside of using large blocks is that RAM is wasted in the block cache.
      block-size: 16KiB
      # This should be kept to null if write-buffer-manager is set,
      # or if you want to use the "memtable-memory-budget-size" logic.
      # Remember that there are "max-write-buffer-number" in memory, 2 by default
      write-buffer-size: 64MiB
      # Enable blob files
      blob-files: false
    }
    column-options: [
      ${database.global.fallback-column-options} {
        name: "default"
      }
    ]
  }
}
